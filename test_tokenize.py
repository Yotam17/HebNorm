#!/usr/bin/env python3
"""
Test the tokenize function from spellcheck.py
"""

import sys
import os

# Add the project root to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '.'))

def test_tokenize():
    """Test the tokenize function with various Hebrew text examples"""
    print("ğŸ§ª Testing tokenize function...")
    
    try:
        from app.utils.spellcheck import HebrewTokenizer
        
        tokenizer = HebrewTokenizer()
        
        # Test cases
        test_cases = [
            # Basic Hebrew text
            ("×©×œ×•× ×¢×•×œ×", ["×©×œ×•×", "×¢×•×œ×"]),
            
            # Text with final letters in correct positions
            ("×× ×™ ×”×•×œ×š ×”×‘×™×ª×”", ["×× ×™", "×”×•×œ×š", "×”×‘×™×ª×”"]),
            
            # Text with final letters and quotes
            ("×”×¡×¤×¨ ×¢×œ ×”×©×•×œ×—×Ÿ", ["×”×¡×¤×¨", "×¢×œ", "×”×©×•×œ×—×Ÿ"]),
            
            # Text with mixed Hebrew and non-Hebrew
            ("×©×œ×•× world ×©×œ×•×", ["×©×œ×•×", "×©×œ×•×"]),
            
            # Text with final letters in wrong positions (should be filtered out)
            ("×©×œ×•× ×¥×¨ ×¢×•×œ×", ["×©×œ×•×", "×¢×•×œ×"]),
            
            # Text with final letters and quotes in correct positions
            ("×©×œ×•× ×¥' ×¢×•×œ×", ["×©×œ×•×", "×¥'", "×¢×•×œ×"]),
            
            # Text with final letters at the end (should be kept)
            ("×©×œ×•× ×¥ ×¢×•×œ×", ["×©×œ×•×", "×¥", "×¢×•×œ×"]),
            
            # Empty text
            ("", []),
            
            # Whitespace only
            ("   ", []),
            
            # Text starting with non-Hebrew
            ("123 ×©×œ×•× ×¢×•×œ×", ["×©×œ×•×", "×¢×•×œ×"]),
            
            # Text with multiple spaces
            ("×©×œ×•×    ×¢×•×œ×", ["×©×œ×•×", "×¢×•×œ×"]),
        ]
        
        all_passed = True
        
        for input_text, expected_output in test_cases:
            try:
                result = tokenizer.tokenize(input_text)
                if result == expected_output:
                    print(f"   âœ… '{input_text}' â†’ {result}")
                else:
                    print(f"   âŒ '{input_text}' â†’ {result} (expected: {expected_output})")
                    all_passed = False
            except Exception as e:
                print(f"   âŒ Error processing '{input_text}': {e}")
                all_passed = False
        
        if all_passed:
            print("   ğŸ‰ All tokenize tests passed!")
        else:
            print("   âš ï¸  Some tokenize tests failed!")
        
        return all_passed
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Tokenize Function Test")
    print("=" * 40)
    
    # Run test
    success = test_tokenize()
    
    print("\n" + "=" * 40)
    if success:
        print("ğŸ‰ Tokenize function is working correctly!")
    else:
        print("âš ï¸  Tokenize function has issues. Check the error messages above.")
    
    print("\nğŸ’¡ The tokenize function now implements all the rules:")
    print("   1. Split by whitespace")
    print("   2. Filter words that don't start with Hebrew letters")
    print("   3. Filter words with final letters in wrong positions")
    print("   4. Keep only valid Hebrew words")
